{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Automate Deploying Model as a Web App v1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/karthikreddykuna/test/blob/main/Automate_Deploying_Model_as_a_Web_App_v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Installation of AI-Starter Library"
      ],
      "metadata": {
        "id": "NldJbuxQLBAh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install DXC-Industrialized-AI-Starter\n",
        "!pip install git+https://github.com/npulagam/DXC-Industrialized-AI-Starter.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "iPsQ2cfsLM24",
        "outputId": "9d911b7a-7b98-4ae5-dd1a-b906ec1353af"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/npulagam/DXC-Industrialized-AI-Starter.git\n",
            "  Cloning https://github.com/npulagam/DXC-Industrialized-AI-Starter.git to /tmp/pip-req-build-0pgpu35n\n",
            "  Running command git clone -q https://github.com/npulagam/DXC-Industrialized-AI-Starter.git /tmp/pip-req-build-0pgpu35n\n",
            "Collecting TPOT==0.11.7\n",
            "  Downloading TPOT-0.11.7-py3-none-any.whl (87 kB)\n",
            "\u001b[K     |████████████████████████████████| 87 kB 3.7 MB/s \n",
            "\u001b[?25hCollecting yellowbrick==1.3\n",
            "  Downloading yellowbrick-1.3-py3-none-any.whl (271 kB)\n",
            "\u001b[K     |████████████████████████████████| 271 kB 15.4 MB/s \n",
            "\u001b[?25hCollecting scrubadub==1.2.2\n",
            "  Downloading scrubadub-1.2.2-py3-none-any.whl (18 kB)\n",
            "Collecting feature_engine==1.1.1\n",
            "  Downloading feature_engine-1.1.1-py2.py3-none-any.whl (179 kB)\n",
            "\u001b[K     |████████████████████████████████| 179 kB 14.3 MB/s \n",
            "\u001b[?25hCollecting pymongo==3.12.0\n",
            "  Downloading pymongo-3.12.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (526 kB)\n",
            "\u001b[K     |████████████████████████████████| 526 kB 43.7 MB/s \n",
            "\u001b[?25hCollecting pmdarima==1.8.2\n",
            "  Downloading pmdarima-1.8.2-cp37-cp37m-manylinux1_x86_64.whl (1.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5 MB 43.9 MB/s \n",
            "\u001b[?25hCollecting ftfy==6.0.3\n",
            "  Downloading ftfy-6.0.3.tar.gz (64 kB)\n",
            "\u001b[K     |████████████████████████████████| 64 kB 1.2 MB/s \n",
            "\u001b[?25hCollecting interpret_community==0.19.3\n",
            "  Downloading interpret_community-0.19.3-py3-none-any.whl (5.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.9 MB 32.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: missingno==0.5.0 in /usr/local/lib/python3.7/dist-packages (from DXC-Industrialized-AI-Starter===v3.1.3-36-gc6f4394) (0.5.0)\n",
            "Collecting arrow==1.1.1\n",
            "  Downloading arrow-1.1.1-py3-none-any.whl (60 kB)\n",
            "\u001b[K     |████████████████████████████████| 60 kB 3.9 MB/s \n",
            "\u001b[?25hCollecting pyjanitor==0.20.0\n",
            "  Downloading pyjanitor-0.20.0-py3-none-any.whl (51 kB)\n",
            "\u001b[K     |████████████████████████████████| 51 kB 16 kB/s \n",
            "\u001b[?25hCollecting pyaf==3.0\n",
            "  Downloading pyaf-3.0-py2.py3-none-any.whl (123 kB)\n",
            "\u001b[K     |████████████████████████████████| 123 kB 43.5 MB/s \n",
            "\u001b[?25hCollecting pandas_profiling==3.0.0\n",
            "  Downloading pandas_profiling-3.0.0-py2.py3-none-any.whl (248 kB)\n",
            "\u001b[K     |████████████████████████████████| 248 kB 41.6 MB/s \n",
            "\u001b[?25hCollecting datacleaner==0.1.5\n",
            "  Downloading datacleaner-0.1.5.tar.gz (6.5 kB)\n",
            "Collecting Algorithmia==1.10.0\n",
            "  Downloading algorithmia-1.10.0-py2.py3-none-any.whl (22 kB)\n",
            "Collecting GitPython==3.1.18\n",
            "  Downloading GitPython-3.1.18-py3-none-any.whl (170 kB)\n",
            "\u001b[K     |████████████████████████████████| 170 kB 43.0 MB/s \n",
            "\u001b[?25hCollecting ipython>=7.16.1\n",
            "  Downloading ipython-7.31.1-py3-none-any.whl (792 kB)\n",
            "\u001b[K     |████████████████████████████████| 792 kB 40.0 MB/s \n",
            "\u001b[?25hCollecting raiwidgets==0.9.4\n",
            "  Downloading raiwidgets-0.9.4-py3-none-any.whl (4.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.2 MB 46.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from DXC-Industrialized-AI-Starter===v3.1.3-36-gc6f4394) (1.0.2)\n",
            "Collecting flatten-json==0.1.13\n",
            "  Downloading flatten_json-0.1.13.tar.gz (11 kB)\n",
            "Collecting sqlalchemy==1.3.24\n",
            "  Downloading SQLAlchemy-1.3.24-cp37-cp37m-manylinux2010_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 48.1 MB/s \n",
            "\u001b[?25hCollecting dnspython==2.1.0\n",
            "  Downloading dnspython-2.1.0-py3-none-any.whl (241 kB)\n",
            "\u001b[K     |████████████████████████████████| 241 kB 55.4 MB/s \n",
            "\u001b[?25hCollecting pytest==6.0.1\n",
            "  Downloading pytest-6.0.1-py3-none-any.whl (270 kB)\n",
            "\u001b[K     |████████████████████████████████| 270 kB 44.7 MB/s \n",
            "\u001b[?25hCollecting algorithmia-api-client==1.5.1\n",
            "  Downloading algorithmia_api_client-1.5.1-py2.py3-none-any.whl (150 kB)\n",
            "\u001b[K     |████████████████████████████████| 150 kB 58.7 MB/s \n",
            "\u001b[?25hCollecting enum-compat\n",
            "  Downloading enum_compat-0.0.3-py3-none-any.whl (1.3 kB)\n",
            "Collecting argparse\n",
            "  Downloading argparse-1.4.0-py2.py3-none-any.whl (23 kB)\n",
            "Collecting algorithmia-adk<1.1,>=1.0.2\n",
            "  Downloading algorithmia_adk-1.0.4-py2.py3-none-any.whl (3.9 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from Algorithmia==1.10.0->DXC-Industrialized-AI-Starter===v3.1.3-36-gc6f4394) (1.15.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from Algorithmia==1.10.0->DXC-Industrialized-AI-Starter===v3.1.3-36-gc6f4394) (2.23.0)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.7/dist-packages (from Algorithmia==1.10.0->DXC-Industrialized-AI-Starter===v3.1.3-36-gc6f4394) (0.10.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from algorithmia-api-client==1.5.1->Algorithmia==1.10.0->DXC-Industrialized-AI-Starter===v3.1.3-36-gc6f4394) (2021.10.8)\n",
            "Requirement already satisfied: urllib3>=1.15 in /usr/local/lib/python3.7/dist-packages (from algorithmia-api-client==1.5.1->Algorithmia==1.10.0->DXC-Industrialized-AI-Starter===v3.1.3-36-gc6f4394) (1.24.3)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from algorithmia-api-client==1.5.1->Algorithmia==1.10.0->DXC-Industrialized-AI-Starter===v3.1.3-36-gc6f4394) (2.8.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from arrow==1.1.1->DXC-Industrialized-AI-Starter===v3.1.3-36-gc6f4394) (3.10.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datacleaner==0.1.5->DXC-Industrialized-AI-Starter===v3.1.3-36-gc6f4394) (1.1.5)\n",
            "Collecting update_checker\n",
            "  Downloading update_checker-0.18.0-py3-none-any.whl (7.0 kB)\n",
            "Collecting statsmodels>=0.11.1\n",
            "  Downloading statsmodels-0.13.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.8 MB 56.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from feature_engine==1.1.1->DXC-Industrialized-AI-Starter===v3.1.3-36-gc6f4394) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.18.2 in /usr/local/lib/python3.7/dist-packages (from feature_engine==1.1.1->DXC-Industrialized-AI-Starter===v3.1.3-36-gc6f4394) (1.19.5)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from ftfy==6.0.3->DXC-Industrialized-AI-Starter===v3.1.3-36-gc6f4394) (0.2.5)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from interpret_community==0.19.3->DXC-Industrialized-AI-Starter===v3.1.3-36-gc6f4394) (21.3)\n",
            "Collecting interpret-core[required]<=0.2.5,>=0.1.20\n",
            "  Downloading interpret_core-0.2.5-py3-none-any.whl (6.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.7 MB 26.1 MB/s \n",
            "\u001b[?25hCollecting shap<=0.39.0,>=0.20.0\n",
            "  Downloading shap-0.39.0.tar.gz (356 kB)\n",
            "\u001b[K     |████████████████████████████████| 356 kB 42.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numba<0.54.0 in /usr/local/lib/python3.7/dist-packages (from interpret_community==0.19.3->DXC-Industrialized-AI-Starter===v3.1.3-36-gc6f4394) (0.51.2)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.7/dist-packages (from missingno==0.5.0->DXC-Industrialized-AI-Starter===v3.1.3-36-gc6f4394) (0.11.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from missingno==0.5.0->DXC-Industrialized-AI-Starter===v3.1.3-36-gc6f4394) (3.2.2)\n",
            "Collecting requests\n",
            "  Downloading requests-2.27.1-py2.py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: jinja2>=2.11.1 in /usr/local/lib/python3.7/dist-packages (from pandas_profiling==3.0.0->DXC-Industrialized-AI-Starter===v3.1.3-36-gc6f4394) (2.11.3)\n",
            "Collecting tangled-up-in-unicode==0.1.0\n",
            "  Downloading tangled_up_in_unicode-0.1.0-py3-none-any.whl (3.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1 MB 42.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.48.2 in /usr/local/lib/python3.7/dist-packages (from pandas_profiling==3.0.0->DXC-Industrialized-AI-Starter===v3.1.3-36-gc6f4394) (4.62.3)\n",
            "Collecting phik>=0.11.1\n",
            "  Downloading phik-0.12.0-cp37-cp37m-manylinux2010_x86_64.whl (675 kB)\n",
            "\u001b[K     |████████████████████████████████| 675 kB 56.9 MB/s \n",
            "\u001b[?25hCollecting htmlmin>=0.1.12\n",
            "  Downloading htmlmin-0.1.12.tar.gz (19 kB)\n",
            "Collecting pydantic>=1.8.1\n",
            "  Downloading pydantic-1.9.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.9 MB 44.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from pandas_profiling==3.0.0->DXC-Industrialized-AI-Starter===v3.1.3-36-gc6f4394) (1.1.0)\n",
            "Collecting visions[type_image_path]==0.7.1\n",
            "  Downloading visions-0.7.1-py3-none-any.whl (102 kB)\n",
            "\u001b[K     |████████████████████████████████| 102 kB 39.7 MB/s \n",
            "\u001b[?25hCollecting PyYAML>=5.0.0\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 47.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools!=50.0.0,>=38.6.0 in /usr/local/lib/python3.7/dist-packages (from pmdarima==1.8.2->DXC-Industrialized-AI-Starter===v3.1.3-36-gc6f4394) (57.4.0)\n",
            "Requirement already satisfied: Cython!=0.29.18,>=0.29 in /usr/local/lib/python3.7/dist-packages (from pmdarima==1.8.2->DXC-Industrialized-AI-Starter===v3.1.3-36-gc6f4394) (0.29.26)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (from pyaf==3.0->DXC-Industrialized-AI-Starter===v3.1.3-36-gc6f4394) (0.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from pyaf==3.0->DXC-Industrialized-AI-Starter===v3.1.3-36-gc6f4394) (0.3.4)\n",
            "Requirement already satisfied: pydot in /usr/local/lib/python3.7/dist-packages (from pyaf==3.0->DXC-Industrialized-AI-Starter===v3.1.3-36-gc6f4394) (1.3.0)\n",
            "Requirement already satisfied: xlrd>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from pyjanitor==0.20.0->DXC-Industrialized-AI-Starter===v3.1.3-36-gc6f4394) (1.1.0)\n",
            "Collecting pandas-flavor==0.1.2\n",
            "  Downloading pandas_flavor-0.1.2-py2.py3-none-any.whl (5.7 kB)\n",
            "Requirement already satisfied: py>=1.8.2 in /usr/local/lib/python3.7/dist-packages (from pytest==6.0.1->DXC-Industrialized-AI-Starter===v3.1.3-36-gc6f4394) (1.11.0)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pytest==6.0.1->DXC-Industrialized-AI-Starter===v3.1.3-36-gc6f4394) (8.12.0)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.7/dist-packages (from pytest==6.0.1->DXC-Industrialized-AI-Starter===v3.1.3-36-gc6f4394) (1.1.1)\n",
            "Requirement already satisfied: importlib-metadata>=0.12 in /usr/local/lib/python3.7/dist-packages (from pytest==6.0.1->DXC-Industrialized-AI-Starter===v3.1.3-36-gc6f4394) (4.10.1)\n",
            "Collecting pluggy<1.0,>=0.12\n",
            "  Downloading pluggy-0.13.1-py2.py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from pytest==6.0.1->DXC-Industrialized-AI-Starter===v3.1.3-36-gc6f4394) (21.4.0)\n",
            "Collecting ipython>=7.16.1\n",
            "  Downloading ipython-7.16.1-py3-none-any.whl (785 kB)\n",
            "\u001b[K     |████████████████████████████████| 785 kB 35.4 MB/s \n",
            "\u001b[?25hCollecting erroranalysis>=0.1.16\n",
            "  Downloading erroranalysis-0.1.30-py3-none-any.whl (25 kB)\n",
            "Collecting rai-core-flask==0.2.3\n",
            "  Downloading rai_core_flask-0.2.3-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: ipykernel<6.0 in /usr/local/lib/python3.7/dist-packages (from raiwidgets==0.9.4->DXC-Industrialized-AI-Starter===v3.1.3-36-gc6f4394) (4.10.1)\n",
            "Requirement already satisfied: lightgbm>=2.0.11 in /usr/local/lib/python3.7/dist-packages (from raiwidgets==0.9.4->DXC-Industrialized-AI-Starter===v3.1.3-36-gc6f4394) (2.2.3)\n",
            "Collecting fairlearn>=0.7.0\n",
            "  Downloading fairlearn-0.7.0-py3-none-any.whl (177 kB)\n",
            "\u001b[K     |████████████████████████████████| 177 kB 50.6 MB/s \n",
            "\u001b[?25hCollecting responsibleai==0.9.4\n",
            "  Downloading responsibleai-0.9.4-py3-none-any.whl (60 kB)\n",
            "\u001b[K     |████████████████████████████████| 60 kB 6.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=7.16.1->DXC-Industrialized-AI-Starter===v3.1.3-36-gc6f4394) (0.7.5)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython>=7.16.1->DXC-Industrialized-AI-Starter===v3.1.3-36-gc6f4394) (0.2.0)\n",
            "Requirement already satisfied: jedi>=0.10 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.16.1->DXC-Industrialized-AI-Starter===v3.1.3-36-gc6f4394) (0.18.1)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=7.16.1->DXC-Industrialized-AI-Starter===v3.1.3-36-gc6f4394) (2.6.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython>=7.16.1->DXC-Industrialized-AI-Starter===v3.1.3-36-gc6f4394) (4.4.2)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.16.1->DXC-Industrialized-AI-Starter===v3.1.3-36-gc6f4394) (5.1.1)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython>=7.16.1->DXC-Industrialized-AI-Starter===v3.1.3-36-gc6f4394) (4.8.0)\n",
            "Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0\n",
            "  Downloading prompt_toolkit-3.0.26-py3-none-any.whl (375 kB)\n",
            "\u001b[K     |████████████████████████████████| 375 kB 49.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2>=2.11.1->pandas_profiling==3.0.0->DXC-Industrialized-AI-Starter===v3.1.3-36-gc6f4394) (2.0.1)\n",
            "Collecting greenlet==0.4.17\n",
            "  Downloading greenlet-0.4.17-cp37-cp37m-manylinux1_x86_64.whl (45 kB)\n",
            "\u001b[K     |████████████████████████████████| 45 kB 2.7 MB/s \n",
            "\u001b[?25hCollecting gevent==20.9.0\n",
            "  Downloading gevent-20.9.0-cp37-cp37m-manylinux2010_x86_64.whl (5.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.5 MB 44.1 MB/s \n",
            "\u001b[?25hCollecting Flask-Cors==3.0.9\n",
            "  Downloading Flask_Cors-3.0.9-py2.py3-none-any.whl (14 kB)\n",
            "Collecting Flask~=1.0.0\n",
            "  Downloading Flask-1.0.4-py2.py3-none-any.whl (92 kB)\n",
            "\u001b[K     |████████████████████████████████| 92 kB 345 kB/s \n",
            "\u001b[?25hCollecting zope.event\n",
            "  Downloading zope.event-4.5.0-py2.py3-none-any.whl (6.8 kB)\n",
            "Collecting zope.interface\n",
            "  Downloading zope.interface-5.4.0-cp37-cp37m-manylinux2010_x86_64.whl (251 kB)\n",
            "\u001b[K     |████████████████████████████████| 251 kB 51.4 MB/s \n",
            "\u001b[?25hCollecting dice-ml<0.7,>=0.6.1\n",
            "  Downloading dice_ml-0.6.1-py3-none-any.whl (231 kB)\n",
            "\u001b[K     |████████████████████████████████| 231 kB 50.8 MB/s \n",
            "\u001b[?25hCollecting networkx<=2.5\n",
            "  Downloading networkx-2.5-py3-none-any.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 41.5 MB/s \n",
            "\u001b[?25hCollecting econml~=0.12.0\n",
            "  Downloading econml-0.12.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1 MB 47.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: textblob==0.15.3 in /usr/local/lib/python3.7/dist-packages (from scrubadub==1.2.2->DXC-Industrialized-AI-Starter===v3.1.3-36-gc6f4394) (0.15.3)\n",
            "Collecting phonenumbers\n",
            "  Downloading phonenumbers-8.12.42-py2.py3-none-any.whl (2.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.6 MB 64.7 MB/s \n",
            "\u001b[?25hCollecting argcomplete\n",
            "  Downloading argcomplete-2.0.0-py2.py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.7/dist-packages (from textblob==0.15.3->scrubadub==1.2.2->DXC-Industrialized-AI-Starter===v3.1.3-36-gc6f4394) (3.2.5)\n",
            "Collecting xgboost>=1.1.0\n",
            "  Downloading xgboost-1.5.2-py3-none-manylinux2014_x86_64.whl (173.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 173.6 MB 8.2 kB/s \n",
            "\u001b[?25hCollecting deap>=1.2\n",
            "  Downloading deap-1.3.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (160 kB)\n",
            "\u001b[K     |████████████████████████████████| 160 kB 53.3 MB/s \n",
            "\u001b[?25hCollecting stopit>=1.1.1\n",
            "  Downloading stopit-1.1.2.tar.gz (18 kB)\n",
            "Collecting multimethod==1.4\n",
            "  Downloading multimethod-1.4-py2.py3-none-any.whl (7.3 kB)\n",
            "Requirement already satisfied: bottleneck in /usr/local/lib/python3.7/dist-packages (from visions[type_image_path]==0.7.1->pandas_profiling==3.0.0->DXC-Industrialized-AI-Starter===v3.1.3-36-gc6f4394) (1.3.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from visions[type_image_path]==0.7.1->pandas_profiling==3.0.0->DXC-Industrialized-AI-Starter===v3.1.3-36-gc6f4394) (7.1.2)\n",
            "Collecting imagehash\n",
            "  Downloading ImageHash-4.2.1.tar.gz (812 kB)\n",
            "\u001b[K     |████████████████████████████████| 812 kB 49.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cycler>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from yellowbrick==1.3->DXC-Industrialized-AI-Starter===v3.1.3-36-gc6f4394) (0.11.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from dice-ml<0.7,>=0.6.1->responsibleai==0.9.4->raiwidgets==0.9.4->DXC-Industrialized-AI-Starter===v3.1.3-36-gc6f4394) (3.1.0)\n",
            "Collecting dowhy\n",
            "  Downloading dowhy-0.7-py3-none-any.whl (152 kB)\n",
            "\u001b[K     |████████████████████████████████| 152 kB 48.1 MB/s \n",
            "\u001b[?25hCollecting sparse\n",
            "  Downloading sparse-0.13.0-py2.py3-none-any.whl (77 kB)\n",
            "\u001b[K     |████████████████████████████████| 77 kB 5.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: click>=5.1 in /usr/local/lib/python3.7/dist-packages (from Flask~=1.0.0->rai-core-flask==0.2.3->raiwidgets==0.9.4->DXC-Industrialized-AI-Starter===v3.1.3-36-gc6f4394) (7.1.2)\n",
            "Requirement already satisfied: Werkzeug>=0.14 in /usr/local/lib/python3.7/dist-packages (from Flask~=1.0.0->rai-core-flask==0.2.3->raiwidgets==0.9.4->DXC-Industrialized-AI-Starter===v3.1.3-36-gc6f4394) (1.0.1)\n",
            "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.7/dist-packages (from Flask~=1.0.0->rai-core-flask==0.2.3->raiwidgets==0.9.4->DXC-Industrialized-AI-Starter===v3.1.3-36-gc6f4394) (1.1.0)\n",
            "Collecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.12->pytest==6.0.1->DXC-Industrialized-AI-Starter===v3.1.3-36-gc6f4394) (3.7.0)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipykernel<6.0->raiwidgets==0.9.4->DXC-Industrialized-AI-Starter===v3.1.3-36-gc6f4394) (5.3.5)\n",
            "Requirement already satisfied: tornado>=4.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel<6.0->raiwidgets==0.9.4->DXC-Industrialized-AI-Starter===v3.1.3-36-gc6f4394) (5.1.1)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.10->ipython>=7.16.1->DXC-Industrialized-AI-Starter===v3.1.3-36-gc6f4394) (0.8.3)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->missingno==0.5.0->DXC-Industrialized-AI-Starter===v3.1.3-36-gc6f4394) (3.0.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->missingno==0.5.0->DXC-Industrialized-AI-Starter===v3.1.3-36-gc6f4394) (1.3.2)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba<0.54.0->interpret_community==0.19.3->DXC-Industrialized-AI-Starter===v3.1.3-36-gc6f4394) (0.34.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datacleaner==0.1.5->DXC-Industrialized-AI-Starter===v3.1.3-36-gc6f4394) (2018.9)\n",
            "Collecting scipy>=1.4.1\n",
            "  Downloading scipy-1.7.3-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (38.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 38.1 MB 1.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->Algorithmia==1.10.0->DXC-Industrialized-AI-Starter===v3.1.3-36-gc6f4394) (2.10)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests->Algorithmia==1.10.0->DXC-Industrialized-AI-Starter===v3.1.3-36-gc6f4394) (2.0.10)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->DXC-Industrialized-AI-Starter===v3.1.3-36-gc6f4394) (3.0.0)\n",
            "Collecting slicer==0.0.7\n",
            "  Downloading slicer-0.0.7-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from shap<=0.39.0,>=0.20.0->interpret_community==0.19.3->DXC-Industrialized-AI-Starter===v3.1.3-36-gc6f4394) (1.3.0)\n",
            "Requirement already satisfied: patsy>=0.5.2 in /usr/local/lib/python3.7/dist-packages (from statsmodels>=0.11.1->feature_engine==1.1.1->DXC-Industrialized-AI-Starter===v3.1.3-36-gc6f4394) (0.5.2)\n",
            "Collecting pydot\n",
            "  Downloading pydot-1.4.2-py2.py3-none-any.whl (21 kB)\n",
            "Requirement already satisfied: sympy>=1.4 in /usr/local/lib/python3.7/dist-packages (from dowhy->econml~=0.12.0->responsibleai==0.9.4->raiwidgets==0.9.4->DXC-Industrialized-AI-Starter===v3.1.3-36-gc6f4394) (1.7.1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.7/dist-packages (from sympy>=1.4->dowhy->econml~=0.12.0->responsibleai==0.9.4->raiwidgets==0.9.4->DXC-Industrialized-AI-Starter===v3.1.3-36-gc6f4394) (1.2.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->dice-ml<0.7,>=0.6.1->responsibleai==0.9.4->raiwidgets==0.9.4->DXC-Industrialized-AI-Starter===v3.1.3-36-gc6f4394) (1.5.2)\n",
            "Requirement already satisfied: PyWavelets in /usr/local/lib/python3.7/dist-packages (from imagehash->visions[type_image_path]==0.7.1->pandas_profiling==3.0.0->DXC-Industrialized-AI-Starter===v3.1.3-36-gc6f4394) (1.2.0)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel<6.0->raiwidgets==0.9.4->DXC-Industrialized-AI-Starter===v3.1.3-36-gc6f4394) (22.3.0)\n",
            "Requirement already satisfied: jupyter-core>=4.6.0 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel<6.0->raiwidgets==0.9.4->DXC-Industrialized-AI-Starter===v3.1.3-36-gc6f4394) (4.9.1)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->ipython>=7.16.1->DXC-Industrialized-AI-Starter===v3.1.3-36-gc6f4394) (0.7.0)\n",
            "Building wheels for collected packages: DXC-Industrialized-AI-Starter, datacleaner, flatten-json, ftfy, htmlmin, shap, stopit, imagehash\n",
            "  Building wheel for DXC-Industrialized-AI-Starter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for DXC-Industrialized-AI-Starter: filename=DXC_Industrialized_AI_Starter-v3.1.3_36_gc6f4394-py3-none-any.whl size=3314231 sha256=fdc119ae4c81d8984185915e31041fa5d01c32a99eaa6586b10ef46850edc0a5\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-i__p_b9m/wheels/cf/a6/d5/ff28b5809732b3c00f4100e474cabd1b5b91cf344843b81ce5\n",
            "\u001b[33m  WARNING: Built wheel for DXC-Industrialized-AI-Starter is invalid: Metadata 1.2 mandates PEP 440 version, but 'v3.1.3-36-gc6f4394' is not\u001b[0m\n",
            "  Building wheel for datacleaner (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for datacleaner: filename=datacleaner-0.1.5-py3-none-any.whl size=7423 sha256=2ee378a2c5dcd0c67f99bca4ac1b45aa807ec29ace2bc206b30463a551febd60\n",
            "  Stored in directory: /root/.cache/pip/wheels/26/8c/85/222a1180e47515576715bcfb1733746bf85c91f27adbeffae7\n",
            "  Building wheel for flatten-json (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for flatten-json: filename=flatten_json-0.1.13-py3-none-any.whl size=7979 sha256=e3d8acde05db19d9b2117d8ee02a76a3f1774c949ab3f652760ec7896e1c573d\n",
            "  Stored in directory: /root/.cache/pip/wheels/e6/b3/2a/beb2ceb72d11bf335f9c2f87aae26981f6744f3fc885cde665\n",
            "  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ftfy: filename=ftfy-6.0.3-py3-none-any.whl size=41933 sha256=ebb8b61717ce82e52bab2b6b7f595c17b1a1ccaaef7ef2a356b61c4e37add44a\n",
            "  Stored in directory: /root/.cache/pip/wheels/19/f5/38/273eb3b5e76dfd850619312f693716ac4518b498f5ffb6f56d\n",
            "  Building wheel for htmlmin (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for htmlmin: filename=htmlmin-0.1.12-py3-none-any.whl size=27098 sha256=38d88020267adcc4e5cbfa605514aecbee1cdbce80d8d905befe759422e66d94\n",
            "  Stored in directory: /root/.cache/pip/wheels/70/e1/52/5b14d250ba868768823940c3229e9950d201a26d0bd3ee8655\n",
            "  Building wheel for shap (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for shap: filename=shap-0.39.0-cp37-cp37m-linux_x86_64.whl size=491645 sha256=8591d791736bde34fbf2015c9f97c803ec68cedc2f2df6ca29e172706b682e99\n",
            "  Stored in directory: /root/.cache/pip/wheels/ca/25/8f/6ae5df62c32651cd719e972e738a8aaa4a87414c4d2b14c9c0\n",
            "  Building wheel for stopit (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for stopit: filename=stopit-1.1.2-py3-none-any.whl size=11956 sha256=6a5c2b0cb3879c810de67f6db8bdda8b361c9e8e9fbd198b5fe1a661bde7db8b\n",
            "  Stored in directory: /root/.cache/pip/wheels/e2/d2/79/eaf81edb391e27c87f51b8ef901ecc85a5363dc96b8b8d71e3\n",
            "  Building wheel for imagehash (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for imagehash: filename=ImageHash-4.2.1-py2.py3-none-any.whl size=295206 sha256=5beb53387024a83787b81113666df8183e8b45bcfe3ea1a124374d780071a33e\n",
            "  Stored in directory: /root/.cache/pip/wheels/4c/d5/59/5e3e297533ddb09407769762985d134135064c6831e29a914e\n",
            "Successfully built datacleaner flatten-json ftfy htmlmin shap stopit imagehash\n",
            "Failed to build DXC-Industrialized-AI-Starter\n",
            "Installing collected packages: scipy, statsmodels, slicer, pydot, prompt-toolkit, networkx, interpret-core, zope.interface, zope.event, tangled-up-in-unicode, sparse, shap, multimethod, ipython, greenlet, Flask, dowhy, visions, smmap, requests, interpret-community, imagehash, gevent, Flask-Cors, erroranalysis, econml, dice-ml, xgboost, update-checker, stopit, sqlalchemy, responsibleai, rai-core-flask, PyYAML, pydantic, pluggy, phonenumbers, phik, pandas-flavor, htmlmin, gitdb, fairlearn, enum-compat, deap, argparse, argcomplete, algorithmia-api-client, algorithmia-adk, yellowbrick, TPOT, scrubadub, raiwidgets, pytest, pymongo, pyjanitor, pyaf, pmdarima, pandas-profiling, GitPython, ftfy, flatten-json, feature-engine, dnspython, datacleaner, arrow, Algorithmia, DXC-Industrialized-AI-Starter\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.4.1\n",
            "    Uninstalling scipy-1.4.1:\n",
            "      Successfully uninstalled scipy-1.4.1\n",
            "  Attempting uninstall: statsmodels\n",
            "    Found existing installation: statsmodels 0.10.2\n",
            "    Uninstalling statsmodels-0.10.2:\n",
            "      Successfully uninstalled statsmodels-0.10.2\n",
            "  Attempting uninstall: pydot\n",
            "    Found existing installation: pydot 1.3.0\n",
            "    Uninstalling pydot-1.3.0:\n",
            "      Successfully uninstalled pydot-1.3.0\n",
            "  Attempting uninstall: prompt-toolkit\n",
            "    Found existing installation: prompt-toolkit 1.0.18\n",
            "    Uninstalling prompt-toolkit-1.0.18:\n",
            "      Successfully uninstalled prompt-toolkit-1.0.18\n",
            "  Attempting uninstall: networkx\n",
            "    Found existing installation: networkx 2.6.3\n",
            "    Uninstalling networkx-2.6.3:\n",
            "      Successfully uninstalled networkx-2.6.3\n",
            "  Attempting uninstall: ipython\n",
            "    Found existing installation: ipython 5.5.0\n",
            "    Uninstalling ipython-5.5.0:\n",
            "      Successfully uninstalled ipython-5.5.0\n",
            "  Attempting uninstall: greenlet\n",
            "    Found existing installation: greenlet 1.1.2\n",
            "    Uninstalling greenlet-1.1.2:\n",
            "      Successfully uninstalled greenlet-1.1.2\n",
            "  Attempting uninstall: Flask\n",
            "    Found existing installation: Flask 1.1.4\n",
            "    Uninstalling Flask-1.1.4:\n",
            "      Successfully uninstalled Flask-1.1.4\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Attempting uninstall: xgboost\n",
            "    Found existing installation: xgboost 0.90\n",
            "    Uninstalling xgboost-0.90:\n",
            "      Successfully uninstalled xgboost-0.90\n",
            "  Attempting uninstall: sqlalchemy\n",
            "    Found existing installation: SQLAlchemy 1.4.31\n",
            "    Uninstalling SQLAlchemy-1.4.31:\n",
            "      Successfully uninstalled SQLAlchemy-1.4.31\n",
            "  Attempting uninstall: PyYAML\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Attempting uninstall: pluggy\n",
            "    Found existing installation: pluggy 0.7.1\n",
            "    Uninstalling pluggy-0.7.1:\n",
            "      Successfully uninstalled pluggy-0.7.1\n",
            "  Attempting uninstall: yellowbrick\n",
            "    Found existing installation: yellowbrick 1.3.post1\n",
            "    Uninstalling yellowbrick-1.3.post1:\n",
            "      Successfully uninstalled yellowbrick-1.3.post1\n",
            "  Attempting uninstall: pytest\n",
            "    Found existing installation: pytest 3.6.4\n",
            "    Uninstalling pytest-3.6.4:\n",
            "      Successfully uninstalled pytest-3.6.4\n",
            "  Attempting uninstall: pymongo\n",
            "    Found existing installation: pymongo 4.0.1\n",
            "    Uninstalling pymongo-4.0.1:\n",
            "      Successfully uninstalled pymongo-4.0.1\n",
            "  Attempting uninstall: pandas-profiling\n",
            "    Found existing installation: pandas-profiling 1.4.1\n",
            "    Uninstalling pandas-profiling-1.4.1:\n",
            "      Successfully uninstalled pandas-profiling-1.4.1\n",
            "    Running setup.py install for DXC-Industrialized-AI-Starter ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33m  DEPRECATION: DXC-Industrialized-AI-Starter was installed using the legacy 'setup.py install' method, because a wheel could not be built for it. A possible replacement is to fix the wheel build issue reported above. You can find discussion regarding this at https://github.com/pypa/pip/issues/8368.\u001b[0m\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "jupyter-console 5.2.0 requires prompt-toolkit<2.0.0,>=1.0.0, but you have prompt-toolkit 3.0.26 which is incompatible.\n",
            "google-colab 1.0.0 requires ipython~=5.5.0, but you have ipython 7.16.1 which is incompatible.\n",
            "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.27.1 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed Algorithmia-1.10.0 DXC-Industrialized-AI-Starter-v3.1.3-36-gc6f4394 Flask-1.0.4 Flask-Cors-3.0.9 GitPython-3.1.18 PyYAML-6.0 TPOT-0.11.7 algorithmia-adk-1.0.4 algorithmia-api-client-1.5.1 argcomplete-2.0.0 argparse-1.4.0 arrow-1.1.1 datacleaner-0.1.5 deap-1.3.1 dice-ml-0.6.1 dnspython-2.1.0 dowhy-0.7 econml-0.12.0 enum-compat-0.0.3 erroranalysis-0.1.30 fairlearn-0.7.0 feature-engine-1.1.1 flatten-json-0.1.13 ftfy-6.0.3 gevent-20.9.0 gitdb-4.0.9 greenlet-0.4.17 htmlmin-0.1.12 imagehash-4.2.1 interpret-community-0.19.3 interpret-core-0.2.5 ipython-7.16.1 multimethod-1.4 networkx-2.5 pandas-flavor-0.1.2 pandas-profiling-3.0.0 phik-0.12.0 phonenumbers-8.12.42 pluggy-0.13.1 pmdarima-1.8.2 prompt-toolkit-3.0.26 pyaf-3.0 pydantic-1.9.0 pydot-1.4.2 pyjanitor-0.20.0 pymongo-3.12.0 pytest-6.0.1 rai-core-flask-0.2.3 raiwidgets-0.9.4 requests-2.27.1 responsibleai-0.9.4 scipy-1.7.3 scrubadub-1.2.2 shap-0.39.0 slicer-0.0.7 smmap-5.0.0 sparse-0.13.0 sqlalchemy-1.3.24 statsmodels-0.13.1 stopit-1.1.2 tangled-up-in-unicode-0.1.0 update-checker-0.18.0 visions-0.7.1 xgboost-1.5.2 yellowbrick-1.3 zope.event-4.5.0 zope.interface-5.4.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "IPython",
                  "argparse",
                  "prompt_toolkit"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from dxc import ai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DBW07akILQ5D",
        "outputId": "75643c95-b9ca-414c-f1b2-69337facab0a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "id": "6efDKhnKHLGm",
        "outputId": "4f92f5c3-2f0e-4e61-af2a-f97947157784"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-00cf07b74dcd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Regression Model"
      ],
      "metadata": {
        "id": "JePPYRhkLat2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get the aggregated piplines data from MongoDB\n",
        "data_layer = {\n",
        "    \"connection_string\": \"mongodb+srv://npulagam2:Pnkr$187@kishorcluster.l3anv.azure.mongodb.net/<dbname>?retryWrites=true&w=majority\",\n",
        "    \"collection_name\": \"Airbnb_NY_Collection\",\n",
        "    \"database_name\": \"Airbnb_NY_DB\"\n",
        "}\n",
        "df = ai.get_data_from_pipeline(data_layer)\n",
        "\n",
        "# TODO: design and run an experiment\n",
        "experiment_design = {\n",
        "    #model options include ['tpot_regression()', 'tpot_classification()']\n",
        "    \"model\": ai.tpot_regression(),\n",
        "    \"labels\": df.price,\n",
        "    \"data\": df,\n",
        "}\n",
        "\n",
        "#config_dict = {'sklearn.ensemble.GradientBoostingRegressor':{}}\n",
        "\n",
        "trained_model = ai.run_experiment(experiment_design, verbose = False, max_time_mins = 5, max_eval_time_mins = 0.04, config_dict = None, warm_start = False, export_pipeline = True, scoring = None)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pYge-mMJ8DDk",
        "outputId": "a27e1e76-c18b-4c2a-ad3c-83159983f4ff"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "r2 Score: 0.1274502086838557\n",
            "\n",
            "Negative mean square error: -39222.900748527194\n",
            "\n",
            "Negative root mean square error: -198.0477234116242\n",
            "\n",
            "explained_variance: 0.12789717100697628\n",
            "\n",
            "Negative_mean_absolute_error: -66.77360267478808\n",
            "\n",
            "Negative_median_absolute_error: -32.8958904109589\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Deploy model as a web app\n",
        "\n",
        "### To-Dos \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "*   Create an account in [github](https://github.com/) and create a [new repository](https://docs.github.com/en/get-started/quickstart/create-a-repo) with your project name.\n",
        "*   Create an account in [streamlit cloud](https://streamlit.io/cloud) and link your GitHub account.\n",
        "\n",
        "\n",
        "After running your experiment function, run the <code>ai.generate_req_files()</code> function. It will generate below files. you can see those files under the files section in your colab notebook.\n",
        "\n",
        "\n",
        "\n",
        "*   __prepared_data.csv__ The data file which is used by the model.\n",
        "*   __encoder.pkl__ If your model uses any of the encoding techniques then this file will be generated otherwise it won't be generated.\n",
        "*   __target_encoder.pkl__ If your model uses any of the encoding techniques for the target variable then this file will be generated otherwise it won't be generated. Mostly this file will be generated for classification type problems. \n",
        "\n",
        "\n",
        "Download the generated files and upload in your respective repository in GitHub without changing the file names.\n",
        "\n"
      ],
      "metadata": {
        "id": "2rkc4JoOMYUx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "github_design = {\n",
        "    \"GitHub_Username\": \"karthikreddykuna\",\n",
        "    \"Repository_Name\": \"Housing-Data\",\n",
        "    \"Branch\": \"master\",\n",
        "    \"Commit_Message\": \"DXC_AI_STARTER_STREAMLIT_CHANGES\",\n",
        "    \"Project_Title\": \"Hello Soundarya for Streamlit\"\n",
        "}\n",
        "\n",
        "publish_model(github_design)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B2nfe1_s5vi4",
        "outputId": "a83f2a1a-1744-4da3-ca32-3f6a6b0695ef"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please enter your GitHub PAT\n",
            "··········\n",
            "Data encoder saved in encoder.pkl file\n",
            "Data saved in prepared_data.csv file\n",
            "Generated requirements.txt file\n",
            "Generated app.py file to build the application\n",
            "Github Push succesful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pat = \"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\""
      ],
      "metadata": {
        "id": "UqPhudzHVByH"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "jVWGreM_UFl5",
        "outputId": "87baea33-1b53-4f12-d28d-90de34dd9ff1"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'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'"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from git import Repo\n",
        "import os.path\n",
        "import git, os, shutil\n",
        "from dxc.ai.global_variables import globals_file\n",
        "import pickle\n",
        "import pandas as pd\n",
        "from getpass import getpass\n",
        "\n",
        "\n",
        "def publish_github(github_design, pat_key):    \n",
        "    #UPLOAD_FOLDER = github_design[\"Repository_Name\"]\n",
        "    #REMOTE_URL = \"https://\" + str(github_design[\"pat_key\"]) + \"@github.com/\" + str(github_design[\"GitHub_Username\"]) + '/' + str(github_design[\"Repository_Name\"]) + \".git\"\n",
        "    REMOTE_URL = \"https://\" + str(pat_key) + \"@github.com/\" + str(github_design[\"GitHub_Username\"]) + '/' + str(github_design[\"Repository_Name\"]) + \".git\"\n",
        "    COMMIT_MSG = github_design[\"Commit_Message\"]\n",
        "    BRANCH = github_design[\"Branch\"]\n",
        "    if not os.path.exists(github_design[\"Repository_Name\"]):\n",
        "      os.makedirs(github_design[\"Repository_Name\"])\n",
        "      new_path = os.path.join(github_design[\"Repository_Name\"])\n",
        "      DIR_NAME = new_path\n",
        "    else:\n",
        "      DIR_NAME = github_design[\"Repository_Name\"]\n",
        "\n",
        "    return DIR_NAME, REMOTE_URL, COMMIT_MSG, BRANCH\n",
        "\n",
        "def publish_model(github_design):\n",
        "\n",
        "    print(\"Please enter your GitHub PAT\")\n",
        "    pat_key = getpass()\n",
        "\n",
        "    DIR_NAME, REMOTE_URL, COMMIT_MSG, BRANCH = publish_github(github_design, pat_key)\n",
        "\n",
        "    # git_operation_clone()\n",
        "    # git_operation_clone.git_clone('')\n",
        "    git_operation_clone(DIR_NAME,REMOTE_URL)\n",
        "\n",
        "    if globals_file.run_experiment_encoder_used:\n",
        "      encoder_path = \"https://github.com/\" + str(github_design[\"GitHub_Username\"]) + '/' + str(github_design[\"Repository_Name\"]) + \"/blob/\" + str(github_design[\"Branch\"]) + \"/encoder.pkl?raw=true\"\n",
        "    else:\n",
        "      encoder_path = ''\n",
        "    if globals_file.run_experiment_target_encoder_used:\n",
        "      target_encoder_path = \"https://github.com/\" + str(github_design[\"GitHub_Username\"]) + '/' + str(github_design[\"Repository_Name\"]) + \"/blob/\" + str(github_design[\"Branch\"]) + \"/target_encoder.pkl?raw=true\"\n",
        "    else:\n",
        "      target_encoder_path = ''\n",
        "    data_path = \"https://raw.githubusercontent.com/\" + str(github_design[\"GitHub_Username\"]) + '/' + str(github_design[\"Repository_Name\"]) + '/' + str(github_design[\"Branch\"]) + \"/prepared_data.csv\"\n",
        "\n",
        "    generate_req_files(github_design)\n",
        "    generate_app_script(github_design, data_path, encoder_path, target_encoder_path, app_title = github_design[\"Project_Title\"])\n",
        "\n",
        "    git_operation_push(DIR_NAME, BRANCH, COMMIT_MSG)\n",
        "\n",
        "def git_operation_clone(DIR_NAME,REMOTE_URL):\n",
        "  try:\n",
        "      if os.path.isdir(DIR_NAME):\n",
        "          shutil.rmtree(DIR_NAME)\n",
        "      os.mkdir(DIR_NAME)\n",
        "      repo = git.Repo.init(DIR_NAME)\n",
        "      origin = repo.create_remote('origin', REMOTE_URL)\n",
        "      origin.fetch()\n",
        "      origin.pull(origin.refs[0].remote_head)\n",
        "  except Exception as e:\n",
        "      print(str(e))\n",
        "\n",
        "def git_operation_push(DIR_NAME, BRANCH, COMMIT_MSG):\n",
        "    try:\n",
        "        repo = Repo(DIR_NAME)\n",
        "        try:\n",
        "          repo.git.checkout('-b', BRANCH)\n",
        "        except:\n",
        "          repo.git.checkout(BRANCH)\n",
        "        repo.git.add('--all')\n",
        "        #repo.git.add('/content/Housing-Data/app.py')\n",
        "        #repo.git.add('/content/Housing-Data/encoder.pkl')\n",
        "        repo.index.commit(COMMIT_MSG)\n",
        "        origin = repo.remote('origin')\n",
        "        origin.push(BRANCH)\n",
        "        repo.git.add(update=True)\n",
        "        print(\"Github Push succesful\")\n",
        "    except Exception as e:\n",
        "        print(str(e))\n",
        "\n",
        "def generate_req_files(github_design):\n",
        "    tpot_data = pd.read_csv('/content\\data_file.csv')\n",
        "\n",
        "    # Save encoder\n",
        "    if globals_file.run_experiment_encoder_used:\n",
        "        encoder_save_path = str(github_design[\"Repository_Name\"]) + \"/encoder.pkl\"\n",
        "        pickle.dump(globals_file.run_experiment_encoder, open(encoder_save_path, 'wb'))\n",
        "        print(\"Data encoder saved in encoder.pkl file\")\n",
        "\n",
        "    # Save dataset\n",
        "    data_save_path = str(github_design[\"Repository_Name\"]) + \"/prepared_data.csv\"\n",
        "    tpot_data.to_csv(data_save_path, index=False)\n",
        "    print('Data saved in prepared_data.csv file')\n",
        "\n",
        "    # Save target encoder\n",
        "    if globals_file.run_experiment_target_encoder_used:\n",
        "        target_encoder_save_path = str(github_design[\"Repository_Name\"]) + \"/target_encoder.pkl\"\n",
        "        pickle.dump(globals_file.run_experiment_target_encoder, open(target_encoder_save_path, 'wb'))\n",
        "        print(\"Target encoder saved in target_encoder.pkl file\")\n",
        "\n",
        "\n",
        "def generate_app_script(github_design, data_path='', encoder_path='', target_encoder_path='', app_title='App deployed by AI-Starter'):    \n",
        "    requirements = \"\"\"pip-upgrader\n",
        "pandas\n",
        "requests\n",
        "pickle-mixin\n",
        "scikit-learn\n",
        "streamlit\n",
        "feature-engine\n",
        "    \"\"\"\n",
        "    requirement_save_path = str(github_design[\"Repository_Name\"]) + \"/requirements.txt\"\n",
        "    with open(requirement_save_path, 'w') as f:\n",
        "        f.write(requirements)\n",
        "    print('Generated requirements.txt file')\n",
        "\n",
        "\n",
        "    with open(\"best_pipeline.py\", \"r\") as txt_file:\n",
        "        script = txt_file.readlines()\n",
        "\n",
        "    script = open(\"best_pipeline.py\").read()\n",
        "    script = script.replace('import numpy as np', '')\n",
        "    script = script.replace('import pandas as pd', '')\n",
        "    script = script.replace(\"'/content\\\\data_file.csv\\', sep=\\'COLUMN_SEPARATOR\\', dtype=np.float64\", \"'prepared_data.csv'\")\n",
        "    script = script.replace(\"results = exported_pipeline.predict(testing_features)\", \"\")\n",
        "\n",
        "    #code of app.py\n",
        "    app_script = \"\"\"import streamlit as st\n",
        "import pickle\n",
        "from io import BytesIO\n",
        "import requests\n",
        "import pandas as pd \n",
        "# Code from Best Pipeline.py here\n",
        "best_pipeline\n",
        "######################\n",
        "# User defined values\n",
        "title = 'title of the app'\n",
        "encoder_location = 'encoder.pkl'\n",
        "target_encoder_location = 'target_encode.pkl'\n",
        "if len(encoder_location) > 5:\n",
        "    mfile = BytesIO(requests.get(encoder_location).content)\n",
        "    encoder = pickle.load(mfile)\n",
        "    df = encoder.inverse_transform(features)\n",
        "else:\n",
        "    df = features.copy()\n",
        "if len(target_encoder_location) > 5:\n",
        "    mfile = BytesIO(requests.get(target_encoder_location).content)\n",
        "    target_encoder = pickle.load(mfile)\n",
        "st.title(title)\n",
        "st.sidebar.header('User Input Parameters')\n",
        "st.subheader('User Input parameters')\n",
        "selected_data = dict()\n",
        "for column in df.columns:\n",
        "    if column != 'target':\n",
        "        label = column.replace('_id.','')\n",
        "        label = label.replace('_',' ').title()\n",
        "        if df[column].dtype == 'O':\n",
        "            selected_value = st.sidebar.selectbox(label, list(df[column].unique()))\n",
        "        elif df[column].dtype == 'int64':\n",
        "            selected_value = st.sidebar.number_input(label, min_value=df[column].min(), max_value=df[column].max(), value=df[column].iloc[0], step=1)\n",
        "        elif df[column].dtype == 'float64':\n",
        "            selected_value = st.sidebar.number_input(label, min_value=df[column].min(), max_value=df[column].max(), value=df[column].iloc[0])\n",
        "        \n",
        "        selected_data[column] = selected_value\n",
        "test_data = pd.DataFrame(selected_data, index=[0])\n",
        "st.write(test_data)\n",
        "st.subheader('Prediction')\n",
        "if len(encoder_location) > 5:\n",
        "    test_data = encoder.transform(test_data) \n",
        "prediction = exported_pipeline.predict(test_data)\n",
        "if len(target_encoder_location) > 5:\n",
        "    prediction = target_encoder.inverse_transform(prediction)\n",
        "if 'float' in str(type(prediction[0])):\n",
        "    st.write(round(prediction[0],2))\n",
        "else:\n",
        "    st.write(prediction[0])\n",
        "    \"\"\"\n",
        "\n",
        "    app_script = app_script.replace('best_pipeline', script)\n",
        "    app_script = app_script.replace('encoder.pkl', encoder_path)\n",
        "    app_script = app_script.replace('target_encode.pkl', target_encoder_path)\n",
        "    app_script = app_script.replace('title of the app', app_title)\n",
        "    app_script = app_script.replace('prepared_data.csv', data_path)\n",
        "\n",
        "    app_save_path = str(github_design[\"Repository_Name\"]) + \"/app.py\"\n",
        "    with open(app_save_path, 'w') as f:\n",
        "        f.write(app_script)\n",
        "    print('Generated app.py file to build the application')\n"
      ],
      "metadata": {
        "id": "T2y2gJSD41us"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from dxc.ai.global_variables import globals_file\n",
        "import pickle\n",
        "import pandas as pd\n",
        "\n",
        "def generate_req_files():\n",
        "    tpot_data = pd.read_csv('/content\\data_file.csv')\n",
        "\n",
        "    # Save encoder\n",
        "    if globals_file.run_experiment_encoder_used:\n",
        "        encoder_save_path = str(UPLOAD_FOLDER) + \"/encoder.pkl\"\n",
        "        pickle.dump(globals_file.run_experiment_encoder, open('encoder_save_path, 'wb'))\n",
        "        print(\"Data encoder saved in encoder.pkl file\")\n",
        "\n",
        "    # Save dataset\n",
        "    data_save_path = str(UPLOAD_FOLDER) + \"/prepared_data.csv\"\n",
        "    tpot_data.to_csv(data_save_path, index=False)\n",
        "    print('Data saved in prepared_data.csv file')\n",
        "\n",
        "    # Save target encoder\n",
        "    if globals_file.run_experiment_target_encoder_used:\n",
        "        target_encoder_save_path = str(UPLOAD_FOLDER) + \"/target_encoder.pkl\"\n",
        "        pickle.dump(globals_file.run_experiment_target_encoder, open(target_encoder_save_path, 'wb'))\n",
        "        print(\"Target encoder saved in target_encoder.pkl file\")\n",
        "\n",
        "\n",
        "def generate_app_script(data_path='', encoder_path='', target_encoder_path='', app_title='App deployed by AI-Starter'):    \n",
        "    requirements = \"\"\"pip-upgrader\n",
        "pandas\n",
        "requests\n",
        "pickle-mixin\n",
        "scikit-learn\n",
        "streamlit\n",
        "feature-engine\n",
        "    \"\"\"\n",
        "\n",
        "    with open('requirements.txt', 'w') as f:\n",
        "        f.write(requirements)\n",
        "    print('Generated requirements.txt file')\n",
        "\n",
        "\n",
        "    with open(\"best_pipeline.py\", \"r\") as txt_file:\n",
        "        script = txt_file.readlines()\n",
        "\n",
        "    script = open(\"best_pipeline.py\").read()\n",
        "    script = script.replace('import numpy as np', '')\n",
        "    script = script.replace('import pandas as pd', '')\n",
        "    script = script.replace(\"'/content\\\\data_file.csv\\', sep=\\'COLUMN_SEPARATOR\\', dtype=np.float64\", \"'prepared_data.csv'\")\n",
        "    script = script.replace(\"results = exported_pipeline.predict(testing_features)\", \"\")\n",
        "\n",
        "    #code of app.py\n",
        "    app_script = \"\"\"import streamlit as st\n",
        "import pickle\n",
        "from io import BytesIO\n",
        "import requests\n",
        "import pandas as pd \n",
        "# Code from Best Pipeline.py here\n",
        "best_pipeline\n",
        "######################\n",
        "# User defined values\n",
        "title = 'title of the app'\n",
        "encoder_location = 'encoder.pkl'\n",
        "target_encoder_location = 'target_encode.pkl'\n",
        "if len(encoder_location) > 5:\n",
        "    mfile = BytesIO(requests.get(encoder_location).content)\n",
        "    encoder = pickle.load(mfile)\n",
        "    df = encoder.inverse_transform(features)\n",
        "else:\n",
        "    df = features.copy()\n",
        "if len(target_encoder_location) > 5:\n",
        "    mfile = BytesIO(requests.get(target_encoder_location).content)\n",
        "    target_encoder = pickle.load(mfile)\n",
        "st.title(title)\n",
        "st.sidebar.header('User Input Parameters')\n",
        "st.subheader('User Input parameters')\n",
        "selected_data = dict()\n",
        "for column in df.columns:\n",
        "    if column != 'target':\n",
        "        label = column.replace('_id.','')\n",
        "        label = label.replace('_',' ').title()\n",
        "        if df[column].dtype == 'O':\n",
        "            selected_value = st.sidebar.selectbox(label, list(df[column].unique()))\n",
        "        elif df[column].dtype == 'int64':\n",
        "            selected_value = st.sidebar.number_input(label, min_value=df[column].min(), max_value=df[column].max(), value=df[column].iloc[0], step=1)\n",
        "        elif df[column].dtype == 'float64':\n",
        "            selected_value = st.sidebar.number_input(label, min_value=df[column].min(), max_value=df[column].max(), value=df[column].iloc[0])\n",
        "        \n",
        "        selected_data[column] = selected_value\n",
        "test_data = pd.DataFrame(selected_data, index=[0])\n",
        "st.write(test_data)\n",
        "st.subheader('Prediction')\n",
        "if len(encoder_location) > 5:\n",
        "    test_data = encoder.transform(test_data) \n",
        "prediction = exported_pipeline.predict(test_data)\n",
        "if len(target_encoder_location) > 5:\n",
        "    prediction = target_encoder.inverse_transform(prediction)\n",
        "if 'float' in str(type(prediction[0])):\n",
        "    st.write(round(prediction[0],2))\n",
        "else:\n",
        "    st.write(prediction[0])\n",
        "    \"\"\"\n",
        "\n",
        "    app_script = app_script.replace('best_pipeline', script)\n",
        "    app_script = app_script.replace('encoder.pkl', encoder_path)\n",
        "    app_script = app_script.replace('target_encode.pkl', target_encoder_path)\n",
        "    app_script = app_script.replace('title of the app', app_title)\n",
        "    app_script = app_script.replace('prepared_data.csv', data_path)\n",
        "\n",
        "\n",
        "    with open('app.py', 'w') as f:\n",
        "        f.write(app_script)\n",
        "    print('Generated app.py file to build the application')"
      ],
      "metadata": {
        "id": "z8TCeWwr5pFh"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ai.generate_req_files()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZxVbPhIRK5-X",
        "outputId": "57bfb1d6-19eb-4280-8708-7a61e0d73cbf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data encoder saved in encoder.pkl file\n",
            "Data saved in prepared_data.csv file\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### To-Dos\n",
        "\n",
        "After uploading the generated files into GitHub, copy the respective files paths and place then in below code block. If your model didn't generate either encoder.pkl or target_encoder.pkl then leave 'encoder_path' or 'target_encoder_path' variables as blank.\n",
        "\n",
        "\n",
        "After you update the respective file paths then you can run the <code>ai.generate_app_script()</code> function and it will generate below two files which you can see under the files section in your colab notebook.\n",
        "\n",
        "\n",
        "\n",
        "*   requirements.txt \n",
        "*   app.py\n",
        "\n",
        "\n",
        "Upload these files to your GitHub repository. As a final step go to your streamlit cloud account and deploy your app by providing your GitHub location in streamlit. Fror more information on how to deploy an app [click here](https://docs.streamlit.io/streamlit-cloud/get-started/deploy-an-app). After successful deployment, please update your app link in the notebook. \n"
      ],
      "metadata": {
        "id": "z-4UbJ-EayNs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = 'https://raw.githubusercontent.com/npulagam/Deploy_App/main/AirBnB/prepared_data.csv'\n",
        "encoder_path = 'https://github.com/npulagam/Deploy_App/blob/main/AirBnB/encoder.pkl?raw=true'\n",
        "target_encoder_path = ''\n",
        "app_title = \"Employee Attrition Prediction App\"\n",
        "ai.generate_app_script(data_path, encoder_path, target_encoder_path, app_title)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6keesmyOK6AA",
        "outputId": "978aeb34-ee06-4b4c-cbcd-d904ba670d02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated requirements.txt file\n",
            "Generated app.py file to build the application\n"
          ]
        }
      ]
    }
  ]
}